{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbc5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7eef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aad3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c49371b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff67739a43d1478e816227537fc3449c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9ddc6646664d668e08d12cf1214c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/190k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99037fb55fda4c61b5c5d6d506337e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/204k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db798f833f484eec8f3e48ec750cdd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/55.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a78049d0f2d4decb0170ea9048e086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bcd4442f924d64836eb0804d2e88a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff17658b608247898074bba61b467938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39a6494c2a14b96b22f54f99cac9d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d17a98a9e4b958178e582b75688e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/346k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bf4a0ebf614182b1c9441c5c1ac08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/86.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd0e9da63cb41bc864805ee18a97d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7235c560f949339d8d267d9defd020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817383a5d5ec4a7cb0333d859b66cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "arc_challenge = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
    "arc_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb51b852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7082145',\n",
       " 'question': 'How should a line graph be used to display distance and time data for a moving object?',\n",
       " 'choices': {'text': ['The y-axis should be labeled as time, which is the dependent variable.',\n",
       "   'The y-axis should be labeled as distance, which is the independent variable.',\n",
       "   'The x-axis should be labeled as distance, which is the dependent variable.',\n",
       "   'The x-axis should be labeled as time, which is the independent variable.'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'D'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_challenge['validation'][298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6d1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caff501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a8b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/zhenyub/condaenvs/cenv_x86/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4.0 and may not be 100% compatible with the current version (3.7.6). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "import spacy\n",
    "amrlib.setup_spacy_extension()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ef41a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/zhenyub/condaenvs/cenv_x86/lib/python3.8/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb092dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "access_token = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba0549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697d5d42b37a4126b1431663226922fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f12b46a4bce4eaab94b7e4a10a527fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4505ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein's theory of relativity is composed of two main components: special relativity and general relativity. Special relativity, introduced in 1905, states that the laws of physics are the same for all observers in uniform motion and that the speed of light is always constant, regardless of the observer's frame of reference. General relativity, introduced in 1915, expands on this idea by describing gravity as the curvature of spacetime caused by massive objects, which affects not only objects with mass but also the passage of time itself.\n",
      "Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, introduced in 1905, posits that the laws of physics are the same for all observers in uniform motion relative to one another, and that the speed of light is always constant, regardless of the observer's frame of reference. General relativity, introduced in 1915, expands on this idea by describing gravity as the curvature of spacetime caused by the presence of mass and energy, which affects the motion of objects.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and honest assistant. \\\n",
    "      Respond concisely and truthfully.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Einstein's relativity theory in three sentences.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "response1 = outputs[1][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response1, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f99e9-7406-4a35-ab11-c29deb9ba506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2116dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_IO(question='', candidates='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, give out the answer from the candidates, without any explanations. \\\n",
    "    Here is an example as guideline:\n",
    "    \"Question: What is the capital of France? \\\n",
    "    Candidates: A) London; B) Paris; C) Berlin; D) Madrid; \\\n",
    "    Answer: ###B###\" \\\n",
    "    Make sure to include your answer in the format: ###your choice###. \\\n",
    "    Question: {question} \\\n",
    "    Candidates: {candidates} \\\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c845fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_CoT_0Shot(question='', candidates='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, give out the answer from the candidates, without any explanations. \\\n",
    "    Make sure to include your answer in the format: ###your choice###. \\\n",
    "    Question: {question} \\\n",
    "    Candidates: {candidates} \\\n",
    "    Let's think step by step. \\\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16fd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c04009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '###123###'\n",
    "ast.literal_eval(re.search('###(.*)###', s).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40a430a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3862a73-c59d-4ee9-a602-79a688b42b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:02<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_io = []\n",
    "ori_answers_io = []\n",
    "all_counts = -1\n",
    "for s in tqdm(list(arc_challenge['validation'])[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "\n",
    "    choices = s['choices']['text']\n",
    "    choices_text = ''\n",
    "    i = 65\n",
    "    for c in choices:\n",
    "        choices_text += f'{chr(i)}) {c}; '\n",
    "        i += 1\n",
    "\n",
    "    response = Answer_Agent_IO(ori_question, choices_text, 1)[0]\n",
    "    ori_answers_io.append(response)\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_io.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_io.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_io.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_io.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "538a6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:44<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_cot_0s = []\n",
    "all_counts = -1\n",
    "\n",
    "for s in tqdm(list(arc_challenge['validation'])[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "\n",
    "    choices = s['choices']['text']\n",
    "    choices_text = ''\n",
    "    i = 65\n",
    "    for c in choices:\n",
    "        choices_text += f'{chr(i)}) {c}; '\n",
    "        i += 1\n",
    "\n",
    "    response = Answer_Agent_CoT_0Shot(ori_question, choices_text, 1)[0]\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_cot_0s.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68872c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "417c9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5997eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3981020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baf9e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io_ori.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in ori_answers_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0249c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_0Shot.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot_0s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "870c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_2Shot.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72edb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_golden.txt', 'w') as f:\n",
    "    f.write('\\n'.join(s['answerKey'] for s in list(arc_challenge['validation'])[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe9d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc67c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_golden.txt', 'r') as file:\n",
    "    golden = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b4218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io.txt', 'r') as file:\n",
    "    output_io = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8b8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_0Shot.txt', 'r') as file:\n",
    "    output_cot = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de154aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(output, golden):\n",
    "    correct = 0\n",
    "    length = len(golden)\n",
    "\n",
    "    for (o, g) in zip(output, golden):\n",
    "        o = o[0]\n",
    "        g = g[0]\n",
    "        if o == g:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct, correct/length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ffda979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 0.81)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(output_io, golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58bee115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 0.84)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(output_cot, golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbf119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad673421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_debate(question='', prior=[], others='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, and the reasoning processes from another agent, recheck your thoughts and answer. \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Question: {question} \\\n",
    "    Reasoning from another agent: {others} \\\n",
    "    Thought: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                        \"You are a faithful agent that answer mathematical questions accurately.\"}]\n",
    "    for p in prior:\n",
    "        messages.append({\"role\": \"user\", \"content\": p})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

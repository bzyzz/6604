{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbc5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7eef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aad3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7973c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grade_school_math/data/test.jsonl', 'r') as file_split:\n",
    "    json_list = list(file_split)\n",
    "\n",
    "split = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    split.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92a05de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51b852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6d1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caff501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a8b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/zhenyub/condaenvs/cenv_x86/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4.0 and may not be 100% compatible with the current version (3.7.6). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "import spacy\n",
    "amrlib.setup_spacy_extension()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ef41a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/zhenyub/condaenvs/cenv_x86/lib/python3.8/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb092dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "access_token = \"KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba0549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45deb7c7e5b499cb20e2974666bf36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaf6845b1624214aa522ca876eb2e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8543fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e57180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's world of rapid communication and information exchange. Their importance can be seen in several areas:\n",
      "\n",
      "1. **Response Time**: Fast language models can promptly respond to user queries, providing immediate answers and improving the overall user experience. This is particularly important for applications such as chatbots, virtual assistants, and language translation software.\n",
      "\n",
      "2. **Efficient Processing**: Fast language models can efficiently process large amounts of data, reducing computation time and energy consumption. This allows for more efficient deployment on devices with limited processing power, such as mobile devices or smart home devices.\n",
      "\n",
      "3. **Real-Time Analysis**: Fast language models enable real-time analysis of text data, such as sentiment analysis, topic modeling, and entity recognition. This is crucial for applications like social media monitoring, customer feedback analysis, and news sentiment analysis.\n",
      "\n",
      "4. **Large-Scale Applications**: Fast language models are necessary for large-scale applications like language translation platforms, which require prompt and accurate processing of massive amounts of text data.\n",
      "\n",
      "5. **Specialized Domains**: Fast language models are also important in specialized domains like financial services, healthcare, and education, where fast and accurate processing of text data is critical for tasks like risk analysis, diagnosis, or grading student assignments.\n",
      "\n",
      "Some of the key benefits of fast language models include:\n",
      "\n",
      "- **Improved Customer Experience**: Fast language models enable quick and accurate responses to user queries, leading to improved customer satisfaction and loyalty.\n",
      "- **Increased Productivity**: Fast language models can process large amounts of data quickly, freeing up resources and enabling teams to focus on more strategic tasks.\n",
      "- **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive edge by providing faster, more accurate, and more efficient services.\n",
      "\n",
      "However, achieving fast language models can be challenging, as it requires:\n",
      "\n",
      "- **Specialized Hardware**: Fast language models often rely on specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate processing times.\n",
      "- **Model Optimization**: Language models must be optimized for speed, which can involve techniques like pruning, quantization, and knowledge distillation.\n",
      "- **Data Quality**: Fast language models require high-quality training data to ensure accurate and relevant results.\n",
      "\n",
      "Overall, fast language models are essential for many applications, and investing in their development and deployment can have significant benefits for organizations operating in a fast-paced, data-driven world.\n"
     ]
    }
   ],
   "source": [
    "client = Groq(\n",
    "    api_key='KEY',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4505ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein's theory of relativity is composed of two main components: special relativity and general relativity. Special relativity, introduced in 1905, states that the laws of physics are the same for all observers in uniform motion and that the speed of light is always constant, regardless of the observer's frame of reference. General relativity, introduced in 1915, expands on this idea by describing gravity as the curvature of spacetime caused by massive objects, which affects not only objects with mass but also the passage of time itself.\n",
      "Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, introduced in 1905, posits that the laws of physics are the same for all observers in uniform motion relative to one another, and that the speed of light is always constant, regardless of the observer's frame of reference. General relativity, introduced in 1915, expands on this idea by describing gravity as the curvature of spacetime caused by the presence of mass and energy, which affects the motion of objects.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and honest assistant. \\\n",
    "      Respond concisely and truthfully.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Einstein's relativity theory in three sentences.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "response1 = outputs[1][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response1, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f99e9-7406-4a35-ab11-c29deb9ba506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2116dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_IO(question='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, give out only the direct answer, without any explanations. \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Here are two examples as guideline: \\\n",
    "    \"Question: John bought 3 cars, each worth $30000. How much did John pay in total for his 3 cars? \\\n",
    "    Answer: ###$9000###. \\\n",
    "    Question: Payton has 9 apples that she is planning to evenly split to 3 of her friends. How many apples would each of her friend get? \\\n",
    "    Answer: ###3###.\" \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Question: {question} \\\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages_in = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer mathematical questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages_in,\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "\n",
    "    # responses = []\n",
    "    # n = 0\n",
    "    # while n < num_seq:\n",
    "    #     responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    #     n += 1\n",
    "    \n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4d6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_CoT_2Shot(question='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, give out your chain of thought process and the final answer. \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Here are two examples as guideline: \\\n",
    "    \"Question: John bought 3 cars, each worth $30000. He also bought 3 bikes, each worth $1000. How much did John pay in total? \\\n",
    "    Thought: John paid 3 * $30000 = $90000 for the cars, and 3 *$ 1000 = $3000 for the bikes. The total amounts to $90000 + $3000 = $93000. \\\n",
    "    Answer: ###$93000###. \\\n",
    "    Question: Each day, Payton brings 9 apples that she would split to 3 of her friends. In 3 days, how many apples would each of her friend get in total? \\\n",
    "    Thought: Each of the 3 friends would get 9 / 3 = 3 apples per day. For 3 days, each of her friend would get 3 * 3 = 9 apples in total. \\\n",
    "    Answer: ###9###.\" \\\n",
    "    Make sure to include your final answer in the format: ###answer###. \\\n",
    "    Question: {question} \\\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages_in = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer mathematical questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages_in,\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "\n",
    "    # responses = []\n",
    "    # n = 0\n",
    "    # while n < num_seq:\n",
    "    #     responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    #     n += 1\n",
    "    \n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c537c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_CoT_0Shot(question='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, give out your chain of thought process and the final answer. \\\n",
    "    Make sure to include your answer in the format: ###your answer###. \\\n",
    "    Question: {question} \\\n",
    "    Let's think step by step. \\\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages_in = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer mathematical questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages_in,\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "\n",
    "    # responses = []\n",
    "    # n = 0\n",
    "    # while n < num_seq:\n",
    "    #     responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    #     n += 1\n",
    "    \n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16fd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c04009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '###123###'\n",
    "ast.literal_eval(re.search('###(.*)###', s).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3862a73-c59d-4ee9-a602-79a688b42b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:32<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_io = []\n",
    "ori_answers_io = []\n",
    "all_counts = -1\n",
    "for s in tqdm(split[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "    \n",
    "    response = Answer_Agent_IO(ori_question, 1)\n",
    "    ori_answers_io.append(response)\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_io.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_io.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_io.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_io.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c12dcc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:50<00:00,  5.30s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_cot = []\n",
    "all_counts = -1\n",
    "for s in tqdm(split[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "    \n",
    "    response = Answer_Agent_CoT_2Shot(ori_question, 1)\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_cot.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_cot.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_cot.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_cot.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "352debd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [02:03<04:32,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no answer provided\n",
      "To find out the distance between Henry's first and second stops, let's think step by step.\n",
      "\n",
      "Step 1: First, let's find out the total distance of the trip. The problem already tells us this information: the trip is 60 miles.\n",
      "\n",
      "Step 2: Then, we'll find out how far Henry had traveled when he made each of his stops. For the first stop, he traveled 20 miles. \n",
      "\n",
      "Step 3: For the second stop, we are told it was 15 miles before the end. So we must find out how far from the start the end is - that is 60 miles - then subtract 15. This is written as 60 - 15 = 45. Henry had traveled 45 miles before he made his second stop. \n",
      "\n",
      "Step 4: Now we know the mileage for both the first stop and the second stop. If we subtract the distance traveled at the first stop (20 miles) by the distance traveled at the second stop (45 miles), this will tell us the mileage between the two stops. This is 45 - 20 = 25. \n",
      "\n",
      "The answer is: 25 miles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [04:54<01:54,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no answer provided\n",
      "To solve this problem, I'd follow these steps:\n",
      "\n",
      "1. **Calculate the total number of classes on weekdays:** Judy teaches 5 dance classes per day, and there are 5 weekdays (Monday to Friday), so the total number of classes on weekdays is 5 classes/day * 5 days = 25 classes.\n",
      "\n",
      "2. **Calculate the total number of classes on Saturday:** Judy teaches 8 dance classes on Saturday.\n",
      "\n",
      "3. **Calculate the total number of classes in a week:** Add the total number of classes on weekdays and Saturday, 25 classes + 8 classes = 33 classes.\n",
      "\n",
      "4. **Calculate the total number of students per week:** Each class has 15 students, and there are 33 classes in a week, so the total number of students per week is 15 students/class * 33 classes = 495 students.\n",
      "\n",
      "5. **Calculate the total amount of money earned per week:** Judy charges $15.00 per student, and there are 495 students, so the total amount of money earned per week is $15.00/student * 495 students = $7425.\n",
      "\n",
      "**So, the amount of money Judy makes in 1 week is: $7425.00.**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:54<00:00,  4.15s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_cot_0s = []\n",
    "all_counts = -1\n",
    "for s in tqdm(split[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "    \n",
    "    response = Answer_Agent_CoT_0Shot(ori_question, 1)\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_cot_0s.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8d15a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:49<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_io_ensemble3 = []\n",
    "all_counts = -1\n",
    "for s in tqdm(split[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "    \n",
    "    responses = Answer_Agent_IO(ori_question, 3)\n",
    "    answer_current = []\n",
    "\n",
    "    for response in responses:\n",
    "        try:\n",
    "            response_format = re.search('###(.*)###', response).group(1)\n",
    "            try:\n",
    "                answer_current.append(ast.literal_eval(response_format))\n",
    "            except:\n",
    "                answer_current.append(response_format)\n",
    "        except:\n",
    "            try:\n",
    "                answer_current.append(ast.literal_eval(response))\n",
    "            except:\n",
    "                try:\n",
    "                    response_format = response.split('Answer: ')[1]\n",
    "                    answer_current.append(ast.literal_eval(response_format))\n",
    "                except:\n",
    "                    print('no answer provided')\n",
    "                    print(response)\n",
    "                    answer_current.append('')\n",
    "                    continue\n",
    "    all_answer_lm3_text_io_ensemble3.append(answer_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90addd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$38', '$34', '$34'],\n",
       " [4, 4, 4],\n",
       " ['$106,000', '$98,000', '$110,000'],\n",
       " [1080, 1080, 1080],\n",
       " [65, 65, 75],\n",
       " ['$80', '$96', '$80'],\n",
       " [180, 180, 140],\n",
       " ['(200 * 0.4 * 2) + (200 * 0.6 * 2) + 20', 200, 200],\n",
       " ['distance = 0',\n",
       "  '###distance = 0###',\n",
       "  'John drives 3 hours at 60 mph, so he covers 3 * 60 = 180 miles from home initially. Then, he drives 0.5 hours at 30 mph, so he covers 0.5 * 30 = 15 miles. The remaining distance to home is (4 - 0.5 - 2) * 80 = 1.6 * 80 = 128 miles. So the total distance is 180 + 15 + 128 = 323 miles.'],\n",
       " ['$460', '$460', '$450 + ($10 * 5 * 1.2)'],\n",
       " [180, 240, 180],\n",
       " ['3 * $68 = $204', '3 * $68 = $204', '3 * $68 = $204'],\n",
       " [7, 7, 7],\n",
       " [12, 12, 12],\n",
       " ['60%', '60%', '60%'],\n",
       " ['$150', '$500', '$-500'],\n",
       " [\"Since the question doesn't specify the distance covered by each train, we'll assume they cover the same distance. So, each train would cover 80 + 150 = 230 miles in the two days.\",\n",
       "  'Since the trains travel the same distance, we can find the total distance covered by each train in two days. The total distance covered by each train in the first day is 80 miles and in the second day is 150 miles. So, the total distance covered by each train in the two days is 80 + 150 = 230 miles.',\n",
       "  'Both trains cover 230 miles in total'],\n",
       " ['$170,000', '$770,000', '$170,000'],\n",
       " [24, 24, 24],\n",
       " [8, 6, 24],\n",
       " [16, 14, 14],\n",
       " [14, 9, 13],\n",
       " [6, 8, 6],\n",
       " [8, 6, 8],\n",
       " ['$25.00', '$25.00', '$25'],\n",
       " [5, 6, 6],\n",
       " ['$3 * ($16.50 + $22.50 + $42)', '$3 * ($16.50 + $22.50 + $42)', '$323'],\n",
       " ['$240', '$240', '$240'],\n",
       " [45, 45, 45],\n",
       " ['$66', '$66', 66],\n",
       " [\"Allen's current age is 66, and 10 years from now, Allen's age will be 76\",\n",
       "  \"Allen's current age is 66, and 10 years later, Allen's age will be 76\",\n",
       "  \"Allen's current age is 66, and 10 years from now, Allen's age will be 76\"],\n",
       " [' (80 + (20 + 80)/2 + 80*1.25)/3 = ###  ###59.33',\n",
       "  ' (80 + (20 + 80)/2 + 80*1.25)/3 = ###  ###56.666...',\n",
       "  ' (80 + (20 + 80)/2 + 80*1.25)/3 = ###  ###59.33'],\n",
       " [70, 70, 50],\n",
       " [70, 60, 70],\n",
       " [14, 11, 7],\n",
       " [69, 69, 105],\n",
       " ['$150', '$150', '$150'],\n",
       " [0, 0, 0],\n",
       " [15, 12, 12],\n",
       " [6, 12, 6],\n",
       " [8, 8, 8],\n",
       " [600, 300, 600],\n",
       " [60, 72, 60],\n",
       " [250, 500, 250],\n",
       " ['$20.00', '$20.00', '$20'],\n",
       " ['4 * (5 + 2/5*5 + 2*(2/5*5))',\n",
       "  '5 + 2*(5) + 2*(2*(5)) = 5 + 10 + 20 = 35',\n",
       "  '5 + 7 + 14 = 26'],\n",
       " ['80 + 23 = 103', '80 + 220 - 23 - 80', 117],\n",
       " ['$400', '$600', '$600'],\n",
       " [12, 16, 12],\n",
       " [63, 87, '###(15 * 8) * (1 - 3/4) = 60 * 1/4 = 15'],\n",
       " ['$5880', '$5880', '$5880'],\n",
       " [8, 8, 6],\n",
       " [30, 12, 15],\n",
       " ['$440', '$640', '$640'],\n",
       " [19, 19, 19],\n",
       " [14, 14, 14],\n",
       " [5, 5, 5],\n",
       " [333, 330, 333],\n",
       " ['$76.00', '$76.00', '$76.00'],\n",
       " [206, 676, 206],\n",
       " [18, 18, 18],\n",
       " ['$1400', '$1300', '$1400'],\n",
       " ['$2,500,000', '$2,500,000', '$1,250,000'],\n",
       " ['$2940', '$2940', '$4200'],\n",
       " ['(100/4)*8', '(12/4)*100=300', '(100/4)*8'],\n",
       " [32, 26, 26],\n",
       " [40, 40, 40],\n",
       " ['###answer####  175 + (175 - 35) + 2*(175 - 35) = 175 + 140 + 280 = 595',\n",
       "  325,\n",
       "  325],\n",
       " [15,\n",
       "  12,\n",
       "  'There is not enough information to determine the number of teachers.'],\n",
       " ['$65', '$75', '$75'],\n",
       " ['$18,000', '$10,950.00', '$10,500'],\n",
       " [60, 60, 60],\n",
       " ['$139', '$139', '$139'],\n",
       " ['$825', '$750', '$525'],\n",
       " ['$2 * 12 + $1 * 9 + $2 * 17 = $24 + $9 + $34 = $67',\n",
       "  '$2 x 12 + $1 x 9 + $2 x 17',\n",
       "  '$12*2 + $9*1 + $17*2 = $24 + $9 + $34 = $67'],\n",
       " ['64/4 = 16', 8, '64/4 = 16'],\n",
       " ['(180 * 1 + 365 * 2) / 110 = 7',\n",
       "  \"360 + 770 = 1130, 1130 / 110 = 10.273, round up to 11 since you can't buy a fraction of a bag of dog food, but since the puppy is fed for 365 days in the first year, we must calculate the amount of dog food the puppy is fed in the first year. 180 days at 1 cup a day is 180 cups and 365 - 180 = 185 days at 2 cups a day is 370 cups. So in total the puppy is fed 180 + 370 = 550 cups of dog food in the first year. 550 / 110 = 5\",\n",
       "  110],\n",
       " [120, 200, 200],\n",
       " ['$2.50', '$7.50', '$7.50'],\n",
       " ['$70', '$70', '$70'],\n",
       " [18, 18, 18],\n",
       " [3, '###3', 3],\n",
       " ['4*125 + (4*125 - 2)', '4*125+125-2', 375],\n",
       " [4800, 780, 4800],\n",
       " [12, 12, 12],\n",
       " ['###(34 - 23) * 4 = 44',\n",
       "  '###(34-23)*4### ###112',\n",
       "  '###(34 - 23) * 4### ###112'],\n",
       " [16, '4 + 12 + 6 = 22', '4 + 3*4 + 3*4/2'],\n",
       " ['$8640', '$8400', '$7200'],\n",
       " [(24, 0), (48, 0), (44, 0)],\n",
       " ['$32', '$64', '$32'],\n",
       " [190, 140, 200],\n",
       " [28, 16, 28],\n",
       " [7, 7, 7],\n",
       " ['38 - (38 * 0.1)', 36, 37],\n",
       " [360, 360, 360],\n",
       " [80,\n",
       "  80,\n",
       "  \"### (200 * 3/5) = 120 boys, (200 - 120) = 80 girls. 80 * 2/3 = 53.33 (round down to 53 since you can't have a fraction of a person), so 80 - 53 = 27 girls are not in the girl scout.\"],\n",
       " [7, 7, 7],\n",
       " [24, 24, 24],\n",
       " [15, 15, 25],\n",
       " [16, 16, 116]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_answer_lm3_text_io_ensemble3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68872c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417c9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5997eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3981020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io_70b.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baf9e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io_ori_70b.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in ori_answers_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0249c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_0Shot_70b.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot_0s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "870c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_2Shot_70b.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72edb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_golden.txt', 'w') as f:\n",
    "    f.write('\\n'.join(s['answer'].split('#### ')[1] for s in split[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d825c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f37feb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_RE = re.compile(r\"(\\-?[0-9\\.\\,]+)\")\n",
    "INVALID_ANS = \"[invalid]\"\n",
    "\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return INVALID_ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49c8262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(model_completion, gt_example):\n",
    "    gt_answer = extract_answer(gt_example)\n",
    "    assert gt_answer != INVALID_ANS\n",
    "    return extract_answer(model_completion) == gt_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6e81b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_golden.txt', 'r') as file:\n",
    "    golden = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bf87fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io_70b.txt', 'r') as file:\n",
    "    output_io = file.readlines()\n",
    "\n",
    "with open('answers_lm3_cot_0Shot_70b.txt', 'r') as file:\n",
    "    output_cot_0Shot = file.readlines()\n",
    "\n",
    "with open('answers_lm3_cot_2Shot_70b.txt', 'r') as file:\n",
    "    output_cot_2Shot = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37073565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(output, golden):\n",
    "    correct = 0\n",
    "    length = len(golden)\n",
    "\n",
    "    for (o, g) in zip(output, golden):\n",
    "        res = is_correct(o, g)\n",
    "        if res:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct, correct/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03dbf119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 0.29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(output_io, golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06dbf29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 0.87)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(output_cot_0Shot, golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04b83479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 0.93)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(output_cot_2Shot, golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc81e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad673421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_debate(question='', prior=[], others='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, and the reasoning processes from another agent, recheck your thoughts and answer. \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Question: {question} \\\n",
    "    Reasoning from another agent: {others} \\\n",
    "    Thought: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                        \"You are a faithful agent that answer mathematical questions accurately.\"}]\n",
    "    for p in prior:\n",
    "        messages.append({\"role\": \"user\", \"content\": p})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbc5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7eef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aad3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caff501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ef41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb092dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/zhenyub/condaenvs/cenv_x86/lib/python3.8/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "access_token = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba0549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0007e5041b94d07a87c2e99dbe54476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5c453aceda4a0fa13a2332e8d4ebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f99e9-7406-4a35-ab11-c29deb9ba506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2116dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_IO(input='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Use numbers and basic arithmetic operations (+ - * /) to obtain 24. You can only use each number once. \\\n",
    "        Give out your direct answer only, without any reasoning. \\\n",
    "        Make sure to include ### before and after your answer. \\\n",
    "        Here are some examples: \\\n",
    "        Input: 4 4 6 8 \\\n",
    "        Answer: ###(4 + 8) * (6 - 4) = 12 * 2 = 24### \\\n",
    "        Input: 2 9 10 12 \\\n",
    "        Answer: ###2 * 12 * (10 - 9) = 24 * 1 = 24### \\\n",
    "        Input: 2 4 9 13 \\\n",
    "        Answer: ###(13 - 9) * (2 + 4) = 4 * 6 = 24### \\\n",
    "        Input: 1 4 8 8 \\\n",
    "        Answer: ###(8 / 4 + 1) * 8 = 3 * 8 = 24### \\\n",
    "        Input: 5 5 5 9 \\\n",
    "        Answer: ###5 + 5 + 5 + 9 = 24### \\\n",
    "        Here is the input: \\\n",
    "        Input: {input}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4d439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###(6 + 6) * (6 - 6) = 12 * 0 = 24###',\n",
       " '###(6 + 6 + 6) - 6 = 18 - 6 = 24###',\n",
       " '###(6 + 6) * (6 + 6) = 12 * 12 = 144 / (6 + 6) = 144 / 12 = 12###',\n",
       " '###6 + 6 + 6 = 18, 18 + 6 = 24###',\n",
       " '###(6 + 6 + 6) - 6 = 18 - 6 = 12###',\n",
       " '###(6 + 6) * (6 + 6) = 12 * 12 = 144 / (6 + 6) = 12 / 12 = 1 * 24 = 24###',\n",
       " '###6 + 6 + 6 = 18, 18 + 6 = 24###',\n",
       " '###(6 + 6) * (6 + 6) = 12 * 12 = 144###',\n",
       " '###6 + 6 + 6 = 18, 18 + 6 = 24###',\n",
       " '###6 * (6 + 6) - 6 = 6 * 12 - 6 = 72 - 6 = 66 + 6 - 6 + 6 = 66 + 6 - 6 + 6 = 72 - 6 = 66 + 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 * 6 = 72 / 6 * 6 = 12 *']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer_Agent_IO('6 6 6 6', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c845fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_CoT_0Shot(input, num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Use numbers and basic arithmetic operations (+ - * /) to obtain 24. You can only use each number once. \\\n",
    "        Make sure to include ### before and after your answer. \\\n",
    "        Here are some examples: \\\n",
    "        Input: 4 4 6 8 \\\n",
    "        Answer: ###(4 + 8) * (6 - 4) = 24### \\\n",
    "        Input: 2 9 10 12 \\\n",
    "        Answer: ###2 * 12 * (10 - 9) = 24### \\\n",
    "        Input: 4 9 10 13 \\\n",
    "        Answer: ###(13 - 9) * (10 - 4) = 24### \\\n",
    "        Input: 1 4 8 8 \\\n",
    "        Answer: ###(8 / 4 + 1) * 8 = 24### \\\n",
    "        Input: 5 5 5 9 \\\n",
    "        Answer: ###5 + 5 + 5 + 9 = 24### \\\n",
    "        Here is the input: \\\n",
    "        Input: {input} \\\n",
    "        Let's think step by step.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6a2f1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"### To obtain 24 using the numbers 1, 3, 5, and 7, let's try different combinations.\\n\\nFirst, let's try using the numbers in the order they are given: \\n(1 + 3) * (5 + 7) = 4 * 12 = 48 (not 24)\\n\\nNext, let's try using the numbers in a different order: \\n(1 + 7) * (3 + 5) = 8 * 8 = 64 (not 24)\\n\\nNow, let's try another combination: \\n(1 * 3) * (5 + 7) = 3 * 12 = 36 (not 24)\\n\\nNext, let's try another combination: \\n(1 * 5) * (3 + 7) = 5 * 10 = 50 (not 24)\\n\\nNow, let's try another combination: \\n(1 * 7) * (3 + 5) = 7 * 8 = 56 (not 24)\\n\\nNext, let's try another combination: \\n(3 * 5) * (1 + 7) = 15 * 8 = 120 (not 24)\\n\\nNow, let's try another combination: \\n(3 * 7) * (1 + 5) = 21 * 6 = 126 (not 24)\\n\\nNext, let's try another combination: \\n(5 * 7) * (1 + 3) = 35 * 4 = 140 (not 24)\\n\\nNow, let's try another combination: \\n(1 + 3) * (7 - 5) = 4 * 2 = 8 (not 24)\\n\\nNext, let's try another combination: \\n(1 + 5) * (3 - 7) = 6 * -4 = -24 (not 24)\\n\\nNow, let's try another combination: \\n(1 + 7) * (3 - 5) = 8 * -2 = -16 (not 24)\\n\\nNext, let's try another combination: \\n(3 + 5) * (1 - 7) = 8 * -6 = -48 (not 24)\\n\\nNow, let's try another combination: \\n(3 + 7) * (1 - 5) = 10 * -4 = -40 (not 24)\\n\\nNext, let's try\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer_Agent_CoT_0Shot('1 3 5 7', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f094fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_CoT_3Shot(input, num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Use numbers and basic arithmetic operations (+ - * /) to obtain 24. You can only use each number once. \\\n",
    "        Give out your thinking process step by step. \\\n",
    "        Make sure to include ### before and after your answer. \\\n",
    "        Here are some examples: \\\n",
    "        \"Input: 4 4 6 8 \\\n",
    "        Steps: \\\n",
    "        4 + 8 = 12 (left: 4 6 12) \\\n",
    "        6 - 4 = 2 (left: 2 12) \\\n",
    "        2 * 12 = 24 (left: 24) \\\n",
    "        Answer: ###(6 - 4) * (4 + 8) = 24### \\\n",
    "        Input: 2 9 10 12 \\\n",
    "        Steps: \\\n",
    "        12 * 2 = 24 (left: 9 10 24) \\\n",
    "        10 - 9 = 1 (left: 1 24) \\\n",
    "        24 * 1 = 24 (left: 24) \\\n",
    "        Answer: ###(12 * 2) * (10 - 9) = 24### \\\n",
    "        Input: 4 9 10 13 \\\n",
    "        Steps: \\\n",
    "        13 - 10 = 3 (left: 3 4 9) \\\n",
    "        9 - 3 = 6 (left: 4 6) \\\n",
    "        4 * 6 = 24 (left: 24) \\\n",
    "        Answer: ###4 * (9 - (13 - 10)) = 24###\" \\\n",
    "        Here is the input: \\\n",
    "        Input: {input} \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                     \"You are a faithful agent that answer questions accurately.\"},\\\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f93bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Input: 1 3 5 7\\nSteps:\\n1. 7 - 5 = 2 (left: 1 2 3)\\n2. 3 - 1 = 2 (left: 2 2)\\n3. 2 * 2 = 4 (left: 4)\\n4. 4 * 2 = 8 (left: 8)\\n5. 8 + 2 = 10 (left: 10)\\n6. 10 + 2 = 12 (left: 12)\\n7. 12 + 2 = 14 (left: 14)\\n8. 14 + 2 = 16 (left: 16)\\n9. 16 + 2 = 18 (left: 18)\\n10. 18 + 2 = 20 (left: 20)\\n11. 20 + 2 = 22 (left: 22)\\n12. 22 + 2 = 24 (left: 24)\\n\\nHowever, this process is too long and not efficient. Let's try a different approach.\\n\\nInput: 1 3 5 7\\nSteps:\\n1. 7 - 5 = 2 (left: 1 2 3)\\n2. 3 - 1 = 2 (left: 2 2)\\n3. 7 - 3 = 4 (left: 1 4)\\n4. 4 * 2 = 8 (left: 8)\\n5. 8 + 2 = 10 (left: 10)\\n6. 10 + 2 = 12 (left: 12)\\n7. 12 + 2 = 14 (left: 14)\\n8. 14 + 2 = 16 (left: 16)\\n9. 16 + 2 = 18 (left: 18)\\n10. 18 + 2 = 20 (left: 20)\\n11. 20 + 2 = 22 (left: 22)\\n12. 22 + 2 = 24 (left: 24)\\n\\nHowever, this process is still too long. Let's try a different approach.\\n\\nInput: 1 3 5 7\\nSteps:\\n1. 7 - 5 = 2 (left: 1 2 3)\\n2. 3 - 1 = 2 (left: 2 2)\\n3. \"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answer_Agent_CoT_3Shot('1 3 5 7', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16fd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c04009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '###123###'\n",
    "ast.literal_eval(re.search('###(.*)###', s).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40a430a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3862a73-c59d-4ee9-a602-79a688b42b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:02<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_io = []\n",
    "ori_answers_io = []\n",
    "all_counts = -1\n",
    "for s in tqdm(list(arc_challenge['validation'])[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "\n",
    "    choices = s['choices']['text']\n",
    "    choices_text = ''\n",
    "    i = 65\n",
    "    for c in choices:\n",
    "        choices_text += f'{chr(i)}) {c}; '\n",
    "        i += 1\n",
    "\n",
    "    response = Answer_Agent_IO(ori_question, choices_text, 1)[0]\n",
    "    ori_answers_io.append(response)\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_io.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_io.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_io.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_io.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "538a6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:44<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "all_answer_lm3_text_cot_0s = []\n",
    "all_counts = -1\n",
    "\n",
    "for s in tqdm(list(arc_challenge['validation'])[0:100]):\n",
    "    all_counts += 1\n",
    "    ori_question = s['question']\n",
    "\n",
    "    choices = s['choices']['text']\n",
    "    choices_text = ''\n",
    "    i = 65\n",
    "    for c in choices:\n",
    "        choices_text += f'{chr(i)}) {c}; '\n",
    "        i += 1\n",
    "\n",
    "    response = Answer_Agent_CoT_0Shot(ori_question, choices_text, 1)[0]\n",
    "    try:\n",
    "        response_format = re.search('###(.*)###', response).group(1)\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response_format)\n",
    "        except:\n",
    "            answer_current = response_format\n",
    "        all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "    except:\n",
    "        try:\n",
    "            answer_current = ast.literal_eval(response)\n",
    "            all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "        except:\n",
    "            try:\n",
    "                response_format = response.split('Answer: ')[1]\n",
    "                answer_current = ast.literal_eval(response_format)\n",
    "                all_answer_lm3_text_cot_0s.append(answer_current)\n",
    "            except:\n",
    "                print('no answer provided')\n",
    "                print(response)\n",
    "                all_answer_lm3_text_cot_0s.append('')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68872c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "417c9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5997eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3981020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baf9e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_io_ori.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in ori_answers_io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0249c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_0Shot.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot_0s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "870c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_lm3_cot_2Shot.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(ans) for ans in all_answer_lm3_text_cot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72edb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_golden.txt', 'w') as f:\n",
    "    f.write('\\n'.join(s['answerKey'] for s in list(arc_challenge['validation'])[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbf119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad673421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Answer_Agent_debate(question='', prior=[], others='', num_seq=1):\n",
    "    message = \\\n",
    "    f\"\"\"Given a question, and the reasoning processes from another agent, recheck your thoughts and answer. \\\n",
    "    Make sure to include your answer in the format: ###answer###. \\\n",
    "    Question: {question} \\\n",
    "    Reasoning from another agent: {others} \\\n",
    "    Thought: \\\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \\\n",
    "                        \"You are a faithful agent that answer mathematical questions accurately.\"}]\n",
    "    for p in prior:\n",
    "        messages.append({\"role\": \"user\", \"content\": p})\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_seq,\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    n = 0\n",
    "    while n < num_seq:\n",
    "        responses.append(tokenizer.decode(outputs[n][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "        n += 1\n",
    "    \n",
    "    return responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
